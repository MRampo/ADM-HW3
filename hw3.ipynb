{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection\n",
    "## 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_link = []\n",
    "s = requests.Session()\n",
    "for n in tqdm(range(1,401)):\n",
    "    url = f'https://www.atlasobscura.com/places?page={n}&sort=likes_count'\n",
    "    result = s.get(url)\n",
    "    soup = bs(result.text)\n",
    "    puf = soup.find_all(\"a\", {'class': 'content-card content-card-place'})\n",
    "    for x in puf:\n",
    "        total_link.append(x['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('total_link.txt',\"r\")\n",
    "total_link = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadPage(start,end, array):\n",
    "    count_link = ((start-1)*18)+1\n",
    "    count_page = start\n",
    "    \n",
    "    parent_dir = f'./all_Pages'\n",
    "    s = requests.Session()\n",
    "\n",
    "    for x in tqdm(range(start, end)):\n",
    "        if count_link%10 == 0:\n",
    "            s = requests.Session()\n",
    "        if count_link%18 == 1:\n",
    "            #directory = f'folder_{count_page}'\n",
    "            path = os.path.join(parent_dir, f\"folder_{count_page}\")\n",
    "            os.mkdir(path)\n",
    "        for y in range(18):\n",
    "            url = f'https://www.atlasobscura.com{array[count_link-1]}'\n",
    "            name_file = f'location_{count_link}'\n",
    "            name_folder = f'folder_{count_page}'\n",
    "            with open(f'./all_Pages/{name_folder}/{name_file}.html', 'w', encoding='utf8') as fp:\n",
    "                fp.write(s.get(url).text)\n",
    "            if count_link%18 == 0:\n",
    "                count_page += 1\n",
    "            count_link += 1\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadPage(275, 295, total_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Impossibile creare un file, se il file esiste già: 'TSV Files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matteo\\Desktop\\ADM\\HW3\\ADM-HW3\\hw3.ipynb Cella 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Matteo/Desktop/ADM/HW3/ADM-HW3/hw3.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(\u001b[39m\"\u001b[39;49m\u001b[39mTSV Files\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matteo/Desktop/ADM/HW3/ADM-HW3/hw3.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mall_Pages\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matteo/Desktop/ADM/HW3/ADM-HW3/hw3.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall_Pages/\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Impossibile creare un file, se il file esiste già: 'TSV Files'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"TSV Files\")\n",
    "for x in os.listdir(\"all_Pages\"):\n",
    "    for y in os.listdir(f'all_Pages/{x}'):\n",
    "        with open(f'./all_Pages/{x}/{y}', encoding='utf8') as f:\n",
    "            p = f.read()\n",
    "            soup =  bs(p)\n",
    "            pageAttribute = []\n",
    "            placeName = findPlaceName(soup)\n",
    "            placeTags = findPlaceTags(soup)\n",
    "            numPeopleVisited = findNumPeopleVisited(soup)\n",
    "            numPeopleWant = findNumPeopleWant(soup)\n",
    "            placeDesc = findDescription(soup)\n",
    "            placeShortDesc = findShortDescription(soup)\n",
    "            placeNearby = findNearbyPlaces(soup)\n",
    "            placeAddress = findAddress(soup)\n",
    "            placeAlt, placeLong = findCordinates(soup)\n",
    "            placeEditors = findPostEditors(soup)\n",
    "            placePubDate = findPublishingDate(soup)\n",
    "            placeRelatedList = findPlaceNear(soup)\n",
    "            placeRelatedPlaces = findRelatedPlaces(soup)\n",
    "            placeURL = findPageURL(soup)\n",
    "            with open(f'./TSV Files/{y[:-5]}.tvs', 'wt', encoding='utf8') as fp:\n",
    "                tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "                tsv_writer.writerow([placeName, placeTags, numPeopleVisited, numPeopleWant, placeDesc, placeShortDesc, placeNearby, placeAddress, placeAlt, placeLong, placeEditors, placePubDate, placeRelatedList, placeRelatedPlaces, placeURL])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPlaceName(soup):\n",
    "    placeName = soup.find(\"h1\", {\"class\": \"DDPage__header-title\"})\n",
    "    if placeName != None:\n",
    "        placeName = placeName.text\n",
    "    #placeName = re.sub('[A-Za-z0-9_.,! \"]*' ,'',placeName)\n",
    "    return placeName\n",
    "\n",
    "def findPlaceTags(soup):\n",
    "    tags = []\n",
    "    placeTags = soup.find_all(\"a\", {\"class\": \"itemTags__link js-item-tags-link\"})\n",
    "    for tag in placeTags:\n",
    "        t = tag.text.replace(\"\\n\", \"\")\n",
    "        #t = re.sub('[A-Za-z0-9 _.,!\"]*','',t)\n",
    "        tags.append(t)\n",
    "    return tags\n",
    "\n",
    "def findNumPeopleVisited(soup):\n",
    "    peopleVisited = soup.find_all(\"div\", {\"class\": \"title-md item-action-count\"})\n",
    "    if len(peopleVisited) > 0:\n",
    "        peopleVisited = int(peopleVisited[0].text)\n",
    "    return peopleVisited\n",
    "\n",
    "def findNumPeopleWant(soup):\n",
    "    peopleVisited = soup.find_all(\"div\", {\"class\": \"title-md item-action-count\"})[1]\n",
    "    if len(peopleVisited) > 0:\n",
    "        peopleVisited = int(peopleVisited[1].text)\n",
    "    return peopleVisited\n",
    "\n",
    "def findDescription(soup):\n",
    "    all_description = []\n",
    "    descriptions = soup.find_all(\"div\", {\"class\": \"DDP__body-copy\"})\n",
    "    for description in descriptions:\n",
    "        d = description.text.replace(\"\\n\",\"\")\n",
    "        all_description.append(d)\n",
    "    return all_description\n",
    "\n",
    "def findShortDescription(soup):\n",
    "    shortDescription = soup.find(\"h3\", {\"class\": \"DDPage__header-dek\"})\n",
    "    if shortDescription != None:\n",
    "        shortDescription = shortDescription.text.replace(\"\\n\", \"\")\n",
    "    return shortDescription\n",
    "\n",
    "def findNearbyPlaces(soup):\n",
    "    nearPlaces = []\n",
    "    nearbyPlaces = soup.find_all(\"div\", {\"class\": \"DDPageSiderailRecirc__item-title\"})\n",
    "    if nearbyPlaces != None:\n",
    "        for place in nearbyPlaces:\n",
    "            p = place.text.replace(\"\\n\",\"\")\n",
    "            nearPlaces.append(p)\n",
    "        #Convert the list to set, and then back again to list to remove repetition\n",
    "    return list(set(nearPlaces))\n",
    "\n",
    "def findAddress(soup):\n",
    "    strings = []\n",
    "    adress_strings = soup.find(\"div\", {\"class\":\"DDPageSiderail__column grid-col-lg-4 grid-col-md-5\"}).find(\"aside\", {\"class\": \"DDPageSiderail__details\"}).find(\"address\", {\"class\": \"DDPageSiderail__address\"}).find(\"div\")\n",
    "    for info in adress_strings:\n",
    "        s = info.text.replace(\"\\n\", \"\")\n",
    "        if s != \"\":\n",
    "            strings.append(s)\n",
    "    return \" \".join(strings[:3])\n",
    "\n",
    "def findCordinates(soup):\n",
    "    cordinates = soup.find(\"div\", {\"class\":\"DDPageSiderail__coordinates js-copy-coordinates\"})\n",
    "    if cordinates != None:\n",
    "        placeAlt, placeLong = cordinates.text.replace(\"\\n\", \"\").replace(\" \",\"\").split(\",\")\n",
    "    return placeAlt, placeLong\n",
    "\n",
    "def findPostEditors(soup):\n",
    "    all_editors =[]\n",
    "    editors = soup.find_all(\"a\", {\"class\":\"DDPContributorsList__contributor\"})         \n",
    "    for person in editors:\n",
    "        s = person.text.replace(\"\\n\", \"\")\n",
    "        all_editors.append(s)\n",
    "    return all_editors\n",
    "\n",
    "def findPublishingDate(soup):\n",
    "    #Pick the right info\n",
    "    dateString = soup.find(\"div\", {\"class\":\"DDPContributor__name\"}) \n",
    "    #Let's clean the string\n",
    "    s = dateString.text.replace(\"\\n\", \"\")\n",
    "    #Let's modify it for the right format of datetime\n",
    "    split = s.split()\n",
    "    #Let's convert the string Month into the corrispondent number by using \"strptime()\" \n",
    "    split[0] = str(datetime.datetime.strptime(split[0], '%B').month)\n",
    "    #My format\n",
    "    format = \"%m %d, %Y\"\n",
    "    #Convert from String to datetime\n",
    "    date = datetime.datetime.strptime(\" \".join(split), format)\n",
    "    return date\n",
    "\n",
    "def findPlaceNear(soup):\n",
    "    lists =[]\n",
    "    relatedLists = soup.find(\"div\", {\"data-gtm-template\":\"DDP Footer Recirc Nearby\"})\n",
    "    if relatedLists != None:\n",
    "        relatedLists = relatedLists.find_all(\"h3\", {\"class\":\"Card__heading --content-card-v2-title js-title-content\"})\n",
    "        for list in relatedLists:\n",
    "            s = list.text.replace(\"\\n\", \"\")\n",
    "            #s = re.sub('[A-Za-z0-9 _.,!\"]*','',s)\n",
    "            lists.append(s)\n",
    "    return lists\n",
    "\n",
    "def findRelatedPlaces(soup):\n",
    "    lists =[]\n",
    "    relatedLists = soup.find(\"div\", {\"data-gtm-template\":\"DDP Footer Recirc Related\"})\n",
    "    if relatedLists != None:\n",
    "        relatedLists = relatedLists.find_all(\"h3\", {\"class\":\"Card__heading --content-card-v2-title js-title-content\"})\n",
    "        for list in relatedLists:\n",
    "            s = list.text.replace(\"\\n\", \"\")\n",
    "            #s = re.sub('[A-Za-z0-9 _.,!\"]*','',s)\n",
    "            lists.append(s)\n",
    "    return lists\n",
    "\n",
    "def findPageURL(soup):\n",
    "    numVisitedPeople = soup.find(\"link\", {\"rel\":\"canonical\"})\n",
    "    return numVisitedPeople['href']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from sys import getsizeof\n",
    "for x in os.listdir(\"all_Pages\"):\n",
    "    for y in os.listdir(f\"all_Pages/{x}\"):\n",
    "        with open(f'./all_Pages/{x}/{y}', encoding='utf8') as f:\n",
    "                    p = f.read()\n",
    "                    soup =  bs(p)\n",
    "                    near = findPlaceNear(soup)\n",
    "                    location_tag = soup.find_all('div', {'class':'DDPage__header-place-location'})[0].contents[0]\n",
    "                    if type(location_tag) == bs4.element.NavigableString:\n",
    "                        location = location_tag.text\n",
    "                        print(\"first: \" , location )\n",
    "                    else:\n",
    "                        location = location_tag.contents[0]\n",
    "                        print(location)\n",
    "                        print(\"Type: \", type(location))\n",
    "                        print(\"Size: \" , getsizeof(location) ,\"\\n\")\n",
    "\n",
    "                        location_with_text = location_tag.text\n",
    "                        print(location_with_text)\n",
    "                        print(\"Type: \", type(location_with_text))\n",
    "                        print(\"Size: \", getsizeof(location_with_text), \"\\n\")\n",
    "\n",
    "                        normal_string_location = \"Manhattan, New York\"\n",
    "                        print(normal_string_location)\n",
    "                        print(\"Type: \", type(normal_string_location))\n",
    "                        print(\"Size of normal string with the same content: \", getsizeof(normal_string_location))\n",
    "\n",
    "\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "bs4.element.NavigableString"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd94c265d0cd8d4c5ed894c320907f0fc1771c221568605f48fbe0110aecbb76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
