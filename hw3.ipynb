{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#EDA\n",
    "#do Lemmatization in the Data Cleaning \n",
    "#Create all the missing tsv\n",
    "#Clean and comment the code\n",
    "#CHECK EFFICIENCY 1.2 and 1.3\n",
    "#Parse list before saving into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection\n",
    "## 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_link = []\n",
    "s = requests.Session()\n",
    "for n in tqdm(range(1,401)):\n",
    "    url = f'https://www.atlasobscura.com/places?page={n}&sort=likes_count'\n",
    "    result = s.get(url)\n",
    "    soup = bs(result.text)\n",
    "    puf = soup.find_all(\"a\", {'class': 'content-card content-card-place'})\n",
    "    for x in puf:\n",
    "        total_link.append(x['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('total_link.txt',\"r\")\n",
    "total_link = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadPage(start,end, array):\n",
    "    count_link = ((start-1)*18)+1\n",
    "    count_page = start\n",
    "    \n",
    "    parent_dir = f'./all_Pages'\n",
    "    s = requests.Session()\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    for x in tqdm(range(start, end)):\n",
    "        if count_link%10 == 0:\n",
    "            s = requests.Session()\n",
    "        if count_link%18 == 1:\n",
    "            path = os.path.join(parent_dir, f\"folder_{count_page}\")\n",
    "            os.mkdir(path)\n",
    "        for y in range(18):\n",
    "            url = f'https://www.atlasobscura.com{array[count_link-1]}'\n",
    "            name_file = f'location_{count_link}'\n",
    "            name_folder = f'folder_{count_page}'\n",
    "            with open(f'./all_Pages/{name_folder}/{name_file}.html', 'w', encoding='utf8') as fp:\n",
    "                req = s.get(url, headers = header)\n",
    "                fp.write(req.text)\n",
    "                if req.status_code != 200:\n",
    "                     time.sleep(120)\n",
    "                     req = s.get(url, headers = header)\n",
    "                fp.write(s.get(url).text)\n",
    "            if count_link%18 == 0:\n",
    "                count_page += 1\n",
    "            count_link += 1\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_TSV():\n",
    "    os.mkdir(\"TSV Files\")\n",
    "    for x in os.listdir(\"all_Pages\"):\n",
    "        for y in os.listdir(f'all_Pages/{x}'):\n",
    "            with open(f'./all_Pages/{x}/{y}', encoding='utf8') as f:\n",
    "                p = f.read()\n",
    "                soup =  bs(p)\n",
    "                pageAttribute = []\n",
    "                placeName = findPlaceName(soup)\n",
    "                placeTags = findPlaceTags(soup)\n",
    "                numPeopleVisited = findNumPeopleVisited(soup)\n",
    "                numPeopleWant = findNumPeopleWant(soup)\n",
    "                placeDesc = findDescription(soup)\n",
    "                placeShortDesc = findShortDescription(soup)\n",
    "                placeNearby = findNearbyPlaces(soup)\n",
    "                placeAddress = findAddress(soup)\n",
    "                placeAlt, placeLong = findCordinates(soup)\n",
    "                placeEditors = findPostEditors(soup)\n",
    "                placePubDate = findPublishingDate(soup)\n",
    "                placeRelatedList = findPlaceNear(soup)\n",
    "                placeRelatedPlaces = findRelatedPlaces(soup)\n",
    "                placeURL = findPageURL(soup)\n",
    "                with open(f'./TSV Files/{y[:-5]}.tvs', 'wt', encoding='utf8') as fp:\n",
    "                    csv.writer(fp, delimiter='\\t').writerow([placeName, placeTags, numPeopleVisited, numPeopleWant, placeDesc, placeShortDesc, placeNearby, placeAddress, placeAlt, placeLong, placeEditors, placePubDate, placeRelatedList, placeRelatedPlaces, placeURL])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files, in os.walk(\"all_Pages\"):\n",
    "#     print(files)\n",
    "\n",
    "import glob\n",
    "def download_TSV():\n",
    "    os.mkdir(\"TSV Files\")\n",
    "    a = 0\n",
    "    for path in tqdm(glob.glob(r\"all_Pages/*/*\")):\n",
    "        with open(path, encoding='utf8') as f:\n",
    "                    a += 1\n",
    "                    p = f.read()\n",
    "                    soup =  bs(p)\n",
    "                    pageAttribute = []\n",
    "                    placeName = findPlaceName(soup)\n",
    "                    placeTags = findPlaceTags(soup)\n",
    "                    placeTags = \",\".join(placeTags)\n",
    "                    numPeopleVisited = findNumPeopleVisited(soup)\n",
    "                    numPeopleWant = findNumPeopleWant(soup)\n",
    "                    placeDesc = findDescription(soup)\n",
    "                    placeDesc = \" \".join(placeDesc)\n",
    "                    placeShortDesc = findShortDescription(soup)\n",
    "                    placeNearby = findNearbyPlaces(soup)\n",
    "                    placeNearby = \",\".join(placeNearby)\n",
    "                    placeAddress = findAddress(soup)\n",
    "                    placeAlt, placeLong = findCordinates(soup)\n",
    "                    placeEditors = findPostEditors(soup)\n",
    "                    placeEditors = \"\".join(placeEditors)\n",
    "                    placePubDate = findPublishingDate(soup)\n",
    "                    placeRelatedList = findPlaceNear(soup)\n",
    "                    placeRelatedList = \",\".join(placeRelatedList)\n",
    "                    placeRelatedPlaces = findRelatedPlaces(soup)\n",
    "                    placeRelatedPlaces = \",\".join(placeRelatedPlaces)\n",
    "                    placeURL = findPageURL(soup)\n",
    "                    with open(f'./TSV Files/{a}.tvs', 'wt', encoding='utf8') as fp:\n",
    "                        csv.writer(fp, delimiter='\\t').writerow([placeName, placeTags, numPeopleVisited, numPeopleWant, placeDesc, placeShortDesc, placeNearby, placeAddress, placeAlt, placeLong, placeEditors, placePubDate, placeRelatedList, placeRelatedPlaces, placeURL])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_TSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPlaceName(soup):\n",
    "    placeName = soup.find(\"h1\", {\"class\": \"DDPage__header-title\"})\n",
    "    if placeName != None:\n",
    "        placeName = placeName.text\n",
    "    #placeName = re.sub('[A-Za-z0-9_.,! \"]*' ,'',placeName)\n",
    "    return placeName\n",
    "\n",
    "def findPlaceTags(soup):\n",
    "    tags = []\n",
    "    placeTags = soup.find_all(\"a\", {\"class\": \"itemTags__link js-item-tags-link\"})\n",
    "    for tag in placeTags:\n",
    "        t = tag.text.replace(\"\\n\", \"\")\n",
    "        #t = re.sub('[A-Za-z0-9 _.,!\"]*','',t)\n",
    "        tags.append(t)\n",
    "    return tags\n",
    "\n",
    "def findNumPeopleVisited(soup):\n",
    "    peopleVisited = soup.find_all(\"div\", {\"class\": \"title-md item-action-count\"})\n",
    "    if len(peopleVisited) > 0:\n",
    "        peopleVisited = int(peopleVisited[0].text)\n",
    "    return peopleVisited\n",
    "\n",
    "def findNumPeopleWant(soup):\n",
    "    peopleVisited = soup.find_all(\"div\", {\"class\": \"title-md item-action-count\"})\n",
    "    if len(peopleVisited) > 0:\n",
    "        peopleVisited = int(peopleVisited[1].text)\n",
    "    return peopleVisited\n",
    "\n",
    "def findDescription(soup):\n",
    "    all_description = []\n",
    "    descriptions = soup.find_all(\"div\", {\"class\": \"DDP__body-copy\"})\n",
    "    for description in descriptions:\n",
    "        d = description.text.replace(\"\\n\",\"\")\n",
    "        all_description.append(d)\n",
    "    return all_description\n",
    "\n",
    "def findShortDescription(soup):\n",
    "    shortDescription = soup.find(\"h3\", {\"class\": \"DDPage__header-dek\"})\n",
    "    if shortDescription != None:\n",
    "        shortDescription = shortDescription.text.replace(\"\\n\", \"\")\n",
    "    return shortDescription\n",
    "\n",
    "def findNearbyPlaces(soup):\n",
    "    nearPlaces = []\n",
    "    nearbyPlaces = soup.find_all(\"div\", {\"class\": \"DDPageSiderailRecirc__item-title\"})\n",
    "    if nearbyPlaces != None:\n",
    "        for place in nearbyPlaces:\n",
    "            p = place.text.replace(\"\\n\",\"\")\n",
    "            nearPlaces.append(p)\n",
    "        #Convert the list to set, and then back again to list to remove repetition\n",
    "    return set(nearPlaces)\n",
    "\n",
    "def findAddress(soup):\n",
    "    strings = []\n",
    "    adress_strings = soup.find(\"address\", {\"class\": \"DDPageSiderail__address\"})\n",
    "    if adress_strings != None:\n",
    "        adress_strings = adress_strings.find(\"div\")\n",
    "        for info in adress_strings:\n",
    "            s = info.text.replace(\"\\n\", \"\")\n",
    "            if s != \"\":\n",
    "                strings.append(s)\n",
    "        if len(strings) > 3:\n",
    "            return \" \".join(strings[:3])\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def findCordinates(soup):\n",
    "\n",
    "    cordinates = soup.find(\"div\", {\"class\":\"DDPageSiderail__coordinates js-copy-coordinates\"})\n",
    "    if cordinates != None:\n",
    "        return cordinates.text.replace(\"\\n\", \"\").replace(\" \",\"\").split(\",\")\n",
    "    return \" \", \" \"\n",
    "\n",
    "def findPostEditors(soup):\n",
    "    all_editors =[]\n",
    "    editors = soup.find_all(\"a\", {\"class\":\"DDPContributorsList__contributor\"})         \n",
    "    for person in editors:\n",
    "        s = person.text.replace(\"\\n\", \"\")\n",
    "        all_editors.append(s)\n",
    "    return all_editors\n",
    "\n",
    "def findPublishingDate(soup):\n",
    "    #Pick the right info\n",
    "    dateString = soup.find(\"div\", {\"class\":\"DDPContributor__name\"}) \n",
    "    #Let's clean the string\n",
    "    if dateString != None:\n",
    "        s = dateString.text.replace(\"\\n\", \"\")\n",
    "        #Let's modify it for the right format of datetime\n",
    "        split = s.split()\n",
    "        #Let's convert the string Month into the corrispondent number by using \"strptime()\" \n",
    "        split[0] = str(datetime.datetime.strptime(split[0], '%B').month)\n",
    "        #My format\n",
    "        format = \"%m %d, %Y\"\n",
    "        #Convert from String to datetime\n",
    "        date = datetime.datetime.strptime(\" \".join(split), format)\n",
    "        return date\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def findPlaceNear(soup):\n",
    "    lists =[]\n",
    "    relatedLists = soup.find(\"div\", {\"data-gtm-template\":\"DDP Footer Recirc Nearby\"})\n",
    "    if relatedLists != None:\n",
    "        relatedLists = relatedLists.find_all(\"h3\", {\"class\":\"Card__heading --content-card-v2-title js-title-content\"})\n",
    "        for list in relatedLists:\n",
    "            s = list.text.replace(\"\\n\", \"\")\n",
    "            #s = re.sub('[A-Za-z0-9 _.,!\"]*','',s)\n",
    "            lists.append(s)\n",
    "    return lists\n",
    "\n",
    "def findRelatedPlaces(soup):\n",
    "    lists =[]\n",
    "    relatedLists = soup.find(\"div\", {\"data-gtm-template\":\"DDP Footer Recirc Related\"})\n",
    "    if relatedLists != None:\n",
    "        relatedLists = relatedLists.find_all(\"h3\", {\"class\":\"Card__heading --content-card-v2-title js-title-content\"})\n",
    "        for list in relatedLists:\n",
    "            s = list.text.replace(\"\\n\", \"\")\n",
    "            #s = re.sub('[A-Za-z0-9 _.,!\"]*','',s)\n",
    "            lists.append(s)\n",
    "    return lists\n",
    "\n",
    "def findPageURL(soup):\n",
    "    numVisitedPeople = soup.find(\"link\", {\"rel\":\"canonical\"})\n",
    "    return numVisitedPeople['href']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the tsv file into one pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to save all the .tsv file as one joined .csv where every file rappresents one row of the .csv file, so that we can work and access the data without having to read 7200 tsv file each time. The data are stored in the \"tsv_dataframe\"\n",
    "\n",
    "This mean that the function load_tsv() has only been executed once. The other iteration we just read the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tsv():\n",
    "    tsv = []\n",
    "    dtypes = {}\n",
    "    for x in tqdm(os.listdir(\"TSV Files\")):\n",
    "        df = pd.read_csv(f'TSV Files/{x}',\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"placeName\", \"placeTags\", \"numPeopleVisited\", \"numPeopleWant\", \"placeDesc\", \"placeShortDesc\", \"placeNearby\",\"placeAdress\", \"placeAlt\", \"placeLong\", \"placeEditors\",\"placePubDate\", \"placeRelatedList\", \"placeRelatedPlace\", \"placeURL\"])\n",
    "        tsv.append(df)\n",
    "\n",
    "    return pd.concat(tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_tsv()\n",
    "data = pd.read_csv(\"tsv_dataframe.csv\",index_col=0)\n",
    "\n",
    "#Reset Index\n",
    "data.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- remove punctuation \n",
    "- remove stopwords \n",
    "- stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_punctuations(string):\n",
    "    # first we remove the punctuations\n",
    "    # in order to do it we need to tekenize the string with the function tokenize and then applying the function RegexpTokenizer\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(string)\n",
    "\n",
    "\n",
    "def stemming(string):\n",
    "    # now we move forward with the stemming\n",
    "    porter = PorterStemmer()\n",
    "    string_stem=[porter.stem(word) for word in string]\n",
    "    # we can now return the cleaned string \n",
    "    return string_stem\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    # after this we can now remove all the stopwords in each word in string_t\n",
    "    return  [word for word in string if not word.lower() in set(stopwords.words())]\n",
    "    # now we move forward with the stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prima_riga = data[\"placeDesc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prima_riga =   remove_punctuations(prima_riga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prima_riga =remove_stopwords(prima_riga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prima_riga = stemming(prima_riga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first dictionary where every key is a word and it's value is the corrispondant number to that word\n",
    "\n",
    "def createFirstDic(data):\n",
    "    dic = {}\n",
    "    value = 1\n",
    "    for description in data[\"placeDesc\"]:\n",
    "        #Cleaning the data\n",
    "        description = remove_punctuations(description)\n",
    "        description = remove_stopwords(description)\n",
    "        description = stemming(description)\n",
    "        for word in description:\n",
    "            if word in dic.keys():\n",
    "                continue\n",
    "            else:\n",
    "                dic[word] = value\n",
    "                value += 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 = createFirstDic(data)\n",
    "dic1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Creating your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSecondDic(data, dic1):\n",
    "    dic2 = {}\n",
    "    a = 0\n",
    "    for description in data[\"placeDesc\"]:\n",
    "        #Cleaning the data\n",
    "        description = remove_punctuations(description)\n",
    "        description = remove_stopwords(description)\n",
    "        description = stemming(description)\n",
    "        for word in description:\n",
    "            if dic1[word] in dic2.keys():\n",
    "                dic2[dic1[word]].add(data.placeName[a])\n",
    "            else:\n",
    "                dic2[dic1[word]] = set([data.placeName[a]])\n",
    "            # print(dic2)\n",
    "        a += 1\n",
    "    return dic2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = createSecondDic(data,dic1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(list):\n",
    "    s = dic2[dic1[list[0]]]\n",
    "    for x in range(1, len(list)):\n",
    "        s.intersection(dic2[dic1[list[x]]])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = query([\"class\", \"including\"])\n",
    "\n",
    "res_query = data[data['placeName'].isin(list(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_query.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(txt):\n",
    "    return [word for word in txt.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['list_words'] = data.placeDesc.apply(lambda row: stem_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "x =vectorizer.fit_transform(data.placeDesc).todense()\n",
    "df = pd.DataFrame(x, columns = \n",
    "vectorizer.get_feature_names())\n",
    "df\n",
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInvertedIndex(data, dic1):\n",
    "    dic2 = {}\n",
    "    a = 0\n",
    "    for description in data[\"placeDesc\"]:\n",
    "        description = description.replace('\\'',\"\")\n",
    "        description = description.replace('[',\"\")\n",
    "        description = description.replace(']',\"\")\n",
    "        for word in description.split():\n",
    "            if dic1[word] in dic2.keys():\n",
    "                dic2[dic1[word]].add(data.placeName[a])\n",
    "            else:\n",
    "                b = []\n",
    "                dic2[dic1[word]] = set([data.placeName[a]])\n",
    "            # print(dic2)\n",
    "        a += 1\n",
    "    return dic2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "673b3a28213e3c597c6fa49b945d6bcb4c24a69618c6cbd1a3b12a5f14d6b7d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
